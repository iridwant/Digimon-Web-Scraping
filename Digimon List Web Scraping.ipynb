{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with BeautifulSoup and Writing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I've been given a task to do a **Digimon List** data scraping from this [digimon website](http://digidb.io/digimon-list/), transfering the scraped data into these type of files **(.CSV, .JSON, .XLSX)** and create a databases **(MongoDB and MySQL)** using the libraries that I've learned. \n",
    "\n",
    "The tasks will be broken down into:  \n",
    "1. Scraping data from the website.\n",
    "2. Saving the scraped data into .JSON format.\n",
    "3. Saving the scraped data into .CSV format.\n",
    "4. Saving the scraped data into .XLSX format.\n",
    "5. Creating MySQL databases and storing the scraped data.\n",
    "6. Creating MongoDB databases and storing the scraped data.  \n",
    "\n",
    "Okay, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Scraping Data\n",
    "\n",
    "In this process, I want to scrape the data and transform those data into a list of dictionaries. Why? Because in list of dictionaries format, the next process (saving data into .JSON, .XLSX, .CSV format) will be so much easier. So, the goal in this process will be extracting data from website and transforming it into list of dictionaries.\n",
    "\n",
    "To scrape data from the [digimon website](http://digidb.io/digimon-list/) in Python, we can use these 2 libraries: [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) and [requests](https://requests.readthedocs.io/en/master/). So let's import those 2 libraries (if you haven't install the libraries, please install it first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm gonna specify the url we want to scrape and use *requests* to access the url and test if the url connected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "url = \"http://digidb.io/digimon-list/\"\n",
    "html = requests.get(url)\n",
    "\n",
    "if html.status_code == 200:\n",
    "    print(\"Connected!\")\n",
    "else:\n",
    "    print(html.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I create the *soup* object to use the BeautifulSoup class, and passing the parameter needed in the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the table heading, I can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Digimon', 'Stage', 'Type', 'Attribute', 'Memory', 'Equip Slots', 'HP', 'SP', 'Atk', 'Def', 'Int', 'Spd']\n"
     ]
    }
   ],
   "source": [
    "heading = []\n",
    "\n",
    "for item in soup.find_all('th'):\n",
    "    heading.append(item.text)\n",
    "    \n",
    "print(heading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the table heading above doesn't include the image, and I want to add image column after the '#' column to hold the image link data.  \n",
    "So, I'm gonna do the following syntax to create the 'Image Link' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Image Link', 'Digimon', 'Stage', 'Type', 'Attribute', 'Memory', 'Equip Slots', 'HP', 'SP', 'Atk', 'Def', 'Int', 'Spd']\n"
     ]
    }
   ],
   "source": [
    "heading.insert(1, 'Image Link')\n",
    "\n",
    "print(heading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now I've got all the column header.  \n",
    "Next, I'm gonna scrape the data in the table. In this process, I'm splitting it into two syntax, the first one to get table data and the second one to get the image url.  \n",
    "\n",
    "**Step 1:**   \n",
    "Get the table data and place it in a list variable called **_rawData_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the preview of the first row data: \n",
      "[1, 'Kuramon', 'Baby', 'Free', 'Neutral', 2, 0, 590, 77, 79, 69, 68, 95]\n"
     ]
    }
   ],
   "source": [
    "rawData = []\n",
    "for item in soup.find_all('td'):\n",
    "    if item.text.strip().isdigit(): #to check if the item is number or not, if true cast it into int else let it be\n",
    "        rawData.append(int(item.text.strip()))\n",
    "    else:\n",
    "        rawData.append(item.text.strip())\n",
    "        \n",
    "print(f\"This is the preview of the first row data: \\n{rawData[0:13]}\")\n",
    "\n",
    "#rawData[0:13] are data on the first row after the column title. \n",
    "#Why [0:13]? Because every row consist of 13 column. If you want to grab the second row, you can grab the [13:26], and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:**  \n",
    "Get the image url data and place it in a list called **_img_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the preview of the first row image url data: \n",
      "http://digidb.io/images/dot/dot629.png\n"
     ]
    }
   ],
   "source": [
    "img = []\n",
    "for item in soup.find_all('img', style=\"vertical-align:middle;\"):\n",
    "    img.append(item['src'])\n",
    "    \n",
    "print(f\"This is the preview of the first row image url data: \\n{img[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I get the table data an image url data, the next step I wanna do is to create a dictionary and append it into a list called **_processedData_** to create list of dictionary. The dictionary will be using each of the element in **_heading_** as a key, and for the value it'll be using both of the **_img_** and **_rawData_** elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a preview of the first index in processedData: \n",
      "{'#': 1, 'Image Link': 'http://digidb.io/images/dot/dot629.png', 'Digimon': 'Kuramon', 'Stage': 'Baby', 'Type': 'Free', 'Attribute': 'Neutral', 'Memory': 2, 'Equip Slots': 0, 'HP': 590, 'SP': 77, 'Atk': 79, 'Def': 69, 'Int': 68, 'Spd': 95}\n"
     ]
    }
   ],
   "source": [
    "processedData = [] #variable to keep the dictionary I want to process\n",
    "indexImg = 0 #index counter to keep track of data in img\n",
    "indexRawData = 0 #index counter to keep track of data in rawData\n",
    "addition = 13 #variable used to navigate row in rawData \n",
    "\n",
    "for item in rawData:\n",
    "    while indexRawData < len(rawData):\n",
    "        processedData.append({heading[0]:rawData[indexRawData], heading[1]:img[indexImg], heading[2]:rawData[indexRawData+1], heading[3]:rawData[indexRawData+2], \n",
    "                              heading[4]:rawData[indexRawData+3], heading[5]:rawData[indexRawData+4], heading[6]:rawData[indexRawData+5], heading[7]:rawData[indexRawData+6], \n",
    "                              heading[8]:rawData[indexRawData+7], heading[9]:rawData[indexRawData+8], heading[10]:rawData[indexRawData+9], heading[11]:rawData[indexRawData+10], \n",
    "                              heading[12]:rawData[indexRawData+11], heading[13]:rawData[indexRawData+12]})\n",
    "        indexRawData += addition\n",
    "        indexImg += 1\n",
    "        \n",
    "print(f\"This is a preview of the first index in processedData: \\n{processedData[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, since I've got the data in the list of dictionaries like I want, let's move to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - 4: Saving the Data into .JSON, .CSV, .XLSX Format\n",
    "\n",
    "In this process, I want to save the data I've been scraped into 3 format, which are .JSON, .CSV, and .XLSX. I'll be using 3 libraries for each format, let's do the .JSON format first.  \n",
    "\n",
    "For the import part, I'll import all the library I need in one go, here are the details:  \n",
    " - JSON format: for .JSON format, I'm gonna use [json](https://docs.python.org/3/library/json.html) library.\n",
    " - CSV format: for .CSV format, I'm gonna use [csv](https://docs.python.org/3/library/csv.html) library.\n",
    " - XLSX format: for .XLSX format, I'm gonna use [xlsxwriter](https://xlsxwriter.readthedocs.io/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I already have the data that I wanna write, I just need to specify the JSON file and passing the **_processedData_** variable into **_jsonFile_** and _et voilà!_ Writing to .JSON format is completed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! The file 'digimon.json' has been saved!\n"
     ]
    }
   ],
   "source": [
    "with open(\"digimon.json\",\"w\") as jsonFile:\n",
    "    json.dump(processedData, jsonFile)\n",
    "    \n",
    "print(f\"Success! The file '{jsonFile.name}' has been saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the .CSV format, the process is quite simple too. Because the data that I wanna write is in list of dictionary, I can use DictWriter to write it. I just need to specify the keys which will be the fieldnames, and that's it. The DictWriter will do the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! The file 'digimon.csv' has been saved!\n"
     ]
    }
   ],
   "source": [
    "with open(\"digimon.csv\",\"w\",newline=\"\") as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, delimiter=\";\", fieldnames=list(processedData[0].keys()))\n",
    "    writer.writeheader()\n",
    "    writer.writerows(processedData)\n",
    "    \n",
    "print(f\"Success! The file '{csvFile.name}' has been saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the .XLSX format, I'm gonna use for loop to write out the column header by using dictionary keys, and then I'll use nested for loop to input the table value using dictionary values. But before that, I need to specify the workbook and sheet using **xlsxwriter.Workbook** _(for workbook)_ and **workbook.add_worksheet** _(to add sheet in the workbook)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! The file 'digimon.xlsx' has been saved!\n"
     ]
    }
   ],
   "source": [
    "workbook = xlsxwriter.Workbook(\"digimon.xlsx\")\n",
    "sheet = workbook.add_worksheet(\"Sheet1\")\n",
    "\n",
    "key = list(processedData[0].keys())\n",
    "values = list(processedData[0].values())\n",
    "\n",
    "for col, data in enumerate(key):\n",
    "    sheet.write(0, col, data)\n",
    "\n",
    "for col, item in enumerate(processedData,start = 1): #start at 1 because row 0 are used for column header\n",
    "    for index, data in enumerate(item.values()):\n",
    "        sheet.write(col, index, data)\n",
    "        \n",
    "workbook.close()\n",
    "\n",
    "print(f\"Success! The file '{workbook.filename}' has been saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Storing Data in MySQL Database\n",
    "\n",
    "In this process, I want to store the scraped data into MySQL Database. To do that, I need [mysql connector](https://dev.mysql.com/doc/connector-python/en/) library. So let's import it and move on to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mysql import connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use mysql connector, I need to establish the connection to mysql database first before I can execute any mysql query with python. So I create _my_database_ variable which contains connect class and provide connect function with my connection parameter. After that, I create a variable called _cursor_ which contains cursor class. Cursor class instantiates objects that can execute operations such as SQL statements, I provide it with dictionary=True so the cursor will return it's query as dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_database = connector.connect(host='localhost', port=3306, user='root', passwd='password')\n",
    "\n",
    "cursor = my_database.cursor(dictionary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I establish the connection and setting up the cursor, I can use cursor.execute to process MySQL query from python. Here, I'm creating a database called digiworld, set it as active database and creating a table called digimon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"CREATE DATABASE digiworld\")\n",
    "cursor.execute(\"USE digiworld\")\n",
    "cursor.execute(\"CREATE TABLE digimon(id INT PRIMARY KEY NOT NULL AUTO_INCREMENT,imageLink VARCHAR(255), digimon VARCHAR(255), stage VARCHAR(255), digiType VARCHAR(255),\" \n",
    "              \"attribute VARCHAR(255),memory INT, equipSlots INT, hp INT, sp INT, atk INT, defense INT, intel INT, spd INT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step after creating database and creating table is inserting value into the table that I've just created. I used execute many so I don't need looping to process it. Before I used executemany, I need to provide the value to be executed so I create variable called _val_ which contains a list of dictionary value in processedData. After the cursor running the executemany I have to make the change happened in my database, so I used _database.commit()_ in order to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 records inserted.\n"
     ]
    }
   ],
   "source": [
    "sql = \"INSERT INTO digimon VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "val = [tuple(x.values()) for x in processedData]\n",
    "cursor.executemany(sql, val)\n",
    "\n",
    "my_database.commit()\n",
    "\n",
    "print(cursor.rowcount, \"records inserted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Storing Data in MongoDB Database\n",
    "\n",
    "The last process will be storing the scraped data into MongoDB database. In this part, I'll be using [pymongo](https://api.mongodb.com/python/current/) library to do that. So let's just import it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I've imported the pymongo, I also need to establish the connection first like MySQL, the connection will be called _client_. After I establish the connection, I can create a database by using _client['digiworld']_ and create collection called _digimon_list_. The collection won't be shown until there is at least 1 record in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "urldb = \"mongodb://localhost:27017\"\n",
    "client = MongoClient(urldb)\n",
    "database = client['digiworld']\n",
    "collection = database['digimon_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To insert all the scraped data into collection in one go, I used _collection.insert_many_ and just passed the _processedData_ dictionary without the need to grab the keys or values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 records inserted.\n"
     ]
    }
   ],
   "source": [
    "x = collection.insert_many(processedData)\n",
    "\n",
    "print(f'{len(x.inserted_ids)} records inserted.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
